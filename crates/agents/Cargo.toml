[package]
edition.workspace = true
name              = "moltis-agents"
version.workspace = true

[features]
default                 = ["metrics", "provider-async-openai", "provider-genai"]
local-llm               = ["dep:directories", "dep:encoding_rs", "dep:llama-cpp-2", "dep:sysinfo"]
local-llm-cuda          = ["llama-cpp-2/cuda", "local-llm"]
local-llm-metal         = ["llama-cpp-2/metal", "local-llm"]
metrics                 = ["dep:moltis-metrics"]
provider-async-openai   = ["dep:async-openai"]
provider-genai          = ["dep:genai"]
provider-github-copilot = ["dep:moltis-oauth"]
provider-kimi-code      = ["dep:moltis-oauth"]
provider-openai-codex   = ["dep:base64", "dep:moltis-oauth"]

[dependencies]
anyhow          = { workspace = true }
async-stream    = "0.3"
async-trait     = "0.1"
futures         = { workspace = true }
moltis-common   = { workspace = true }
moltis-config   = { workspace = true }
moltis-sessions = { workspace = true }
moltis-skills   = { workspace = true }
reqwest         = { workspace = true }
secrecy         = { workspace = true }
serde           = { workspace = true }
serde_json      = { workspace = true }
thiserror       = { workspace = true }
tokio           = { workspace = true }
tokio-stream    = { workspace = true }
tracing         = { workspace = true }
uuid            = { workspace = true }

# Optional LLM provider crates
async-openai   = { optional = true, workspace = true }
base64         = { optional = true, workspace = true }
directories    = { optional = true, workspace = true }
encoding_rs    = { optional = true, version = "0.8" }
genai          = { optional = true, workspace = true }
llama-cpp-2    = { optional = true, version = "0.1" }
moltis-metrics = { optional = true, workspace = true }
moltis-oauth   = { optional = true, workspace = true }
sysinfo        = { optional = true, version = "0.34" }

[dev-dependencies]
axum     = { workspace = true }
http     = "1"
tempfile = "3"

[lints]
workspace = true
